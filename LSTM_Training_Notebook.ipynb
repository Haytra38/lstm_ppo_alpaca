{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook d'Entraînement LSTM pour Trading Automatique\n",
    "\n",
    "Ce notebook permet d'entraîner un modèle LSTM avec tous les paramètres configurables pour l'utilisation avec PPO.\n",
    "\n",
    "## Objectifs:\n",
    "1. Charger et visualiser les données de trading\n",
    "2. Configurer le modèle LSTM avec tous les paramètres disponibles\n",
    "3. Entraîner le modèle avec monitoring des performances\n",
    "4. Tester le modèle et préparer l'intégration PPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothèques nécessaires\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Ajout du chemin du projet\n",
    "sys.path.append('.')\n",
    "\n",
    "print(\"\\ud83d\\udd27 Bibliothèques importées avec succès\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration et Chargement des Données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des clés API (chargement depuis variables d'environnement)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Vérification des clés API\n",
    "api_key = os.getenv('ALPACA_API_KEY')\n",
    "api_secret = os.getenv('ALPACA_API_SECRET')\n",
    "\n",
    "if not api_key or not api_secret:\n",
    "    print(\"\\u26a0\\ufe0f  Attention: Clés API non trouvées, utilisation des valeurs par défaut\")\n",
    "    api_key = 'PKWDXUMR64LL03LXOZFN'\n",
    "    api_secret = 'RLtrY3Idh1vQqNizfUJRoJprRJQ9uijfIoLLW4XA'\n",
    "else:\n",
    "    print(\"\\u2705 Clés API chargées avec succès\")\n",
    "\n",
    "# Configuration des données\n",
    "TICKER = 'AAPL'\n",
    "START_DATE = '2023-01-01'\n",
    "END_DATE = '2024-01-01'\n",
    "INTERVAL = '1Min'\n",
    "\n",
    "print(f\"\\ud83d\\udcca Configuration: {TICKER} de {START_DATE} à {END_DATE} ({INTERVAL})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "from data_loader import DataLoader\n",
    "\n",
    "# Initialisation du chargeur de données\n",
    "data_loader = DataLoader(api_key, api_secret)\n",
    "\n",
    "try:\n",
    "    # Chargement des données historiques\n",
    "    print(\"\\ud83d\\udcc1 Chargement des données...\")\n",
    "    df = data_loader.get_minute_data(TICKER, START_DATE, END_DATE)\n",
    "    \n",
    "    if df is not None and not df.empty:\n",
    "        print(f\"\\u2705 Données chargées: {len(df)} lignes\")\n",
    "        print(f\"Période: {df.index.min()} à {df.index.max()}\")\n",
    "        print(f\"Colonnes: {list(df.columns)}\")\n",
    "    else:\n",
    "        print(\"\\u274c Erreur: Aucune donnée chargée\")\n",
    "        # Création de données simulées pour la démonstration\n",
    "        print(\"\\ud83d\\udcca Création de données simulées pour la démonstration...\")\n",
    "        dates = pd.date_range(start=START_DATE, end=END_DATE, freq='1min')\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Génération de données de prix réalistes\n",
    "        returns = np.random.normal(0.0001, 0.02, len(dates))\n",
    "        prices = 100 * np.exp(np.cumsum(returns))\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'open': prices * (1 + np.random.normal(0, 0.001, len(dates))),\n",
    "            'high': prices * (1 + np.abs(np.random.normal(0, 0.002, len(dates)))),\n",
    "            'low': prices * (1 - np.abs(np.random.normal(0, 0.002, len(dates)))),\n",
    "            'close': prices,\n",
    "            'volume': np.random.randint(1000, 10000, len(dates))\n",
    "        }, index=dates)\n",
    "        \n",
    "        print(f\"\\u2705 Données simulées créées: {len(df)} lignes\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\u274c Erreur lors du chargement: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation rapide des données\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle(f'Données {TICKER} - Aper\\u00e7u', fontsize=16)\n",
    "\n",
    "# Prix de clôture\n",
    "axes[0,0].plot(df.index, df['close'], linewidth=0.8, alpha=0.8)\n",
    "axes[0,0].set_title('Prix de Clôture')\n",
    "axes[0,0].set_ylabel('Prix ($)')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volume\n",
    "axes[0,1].bar(df.index[::100], df['volume'][::100], width=0.8, alpha=0.7)\n",
    "axes[0,1].set_title('Volume')\n",
    "axes[0,1].set_ylabel('Volume')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Returns\n",
    "returns = df['close'].pct_change()\n",
    "axes[1,0].hist(returns.dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1,0].set_title('Distribution des Returns')\n",
    "axes[1,0].set_xlabel('Return')\n",
    "axes[1,0].set_ylabel('Fréquence')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volatilité glissante\n",
    "rolling_vol = returns.rolling(window=60).std() * np.sqrt(252*390)\n",
    "axes[1,1].plot(df.index, rolling_vol, linewidth=0.8, alpha=0.8, color='red')\n",
    "axes[1,1].set_title('Volatilité Glissante (60 périodes)')\n",
    "axes[1,1].set_ylabel('Volatilité Annualisée')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques descriptives\n",
    "print(\"\\ud83d\\udcca Statistiques des Données:\")\n",
    "print(f\"Prix moyen: ${df['close'].mean():.2f}\")\n",
    "print(f\"Volatilité daily: {returns.std() * np.sqrt(252*390):.4f}\")\n",
    "print(f\"Sharpe ratio: {(returns.mean() * 252*390) / (returns.std() * np.sqrt(252*390)):.4f}\")\n",
    "print(f\"Max drawdown: {((df['close'] / df['close'].cummax()) - 1).min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration du Modèle LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import du configurateur LSTM\n",
    "from lstm_config import LSTMConfig\n",
    "\n",
    "# Initialisation du configurateur\n",
    "config = LSTMConfig()\n",
    "\n",
    "# Configuration de base\n",
    "print(\"\\ud83d\\udd27 Configuration de base du LSTM...\")\n",
    "\n",
    "# Configuration des données\n",
    "config.set_data_config(\n",
    "    sequence_length=60,\n",
    "    prediction_horizon=5,\n",
    "    features=['close', 'volume', 'returns'],\n",
    "    normalize=True,\n",
    "    technical_indicators=['rsi', 'macd', 'bollinger_bands']\n",
    ")\n",
    "\n",
    "# Configuration du modèle (mode simple)\n",
    "config.set_model_config(\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.2,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Configuration d'entraînement\n",
    "config.set_training_config(\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    early_stopping_patience=10\n",
    ")\n",
    "\n",
    "print(\"\\u2705 Configuration de base appliquée\")\n",
    "config.print_config_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration avancée (optionnelle) - décommentez pour utiliser\n",
    "print(\"\\ud83d\\udd27 Configuration avancée disponible:\")\n",
    "\n",
    "# Passage en mode avancé pour accéder à tous les paramètres\n",
    "# config.set_advanced_mode(True)\n",
    "\n",
    "# Exemple de configuration avancée:\n",
    "if False:  # Passer à True pour activer la config avancée\n",
    "    config.set_advanced_config(\n",
    "        # Architecture LSTM\n",
    "        bidirectional=True,\n",
    "        batch_normalization=True,\n",
    "        layer_normalization=True,\n",
    "        activation_functions=['tanh', 'relu', 'sigmoid'],\n",
    "        \n",
    "        # Dropout et régularisation\n",
    "        dropout_type='variational',  # 'standard', 'variational', 'alpha'\n",
    "        recurrent_dropout=0.3,\n",
    "        weight_decay=1e-4,\n",
    "        \n",
    "        # Optimiseur\n",
    "        optimizer='adamw',  # 'adam', 'adamw', 'rmsprop', 'sgd'\n",
    "        learning_rate_schedule='cosine',  # 'constant', 'step', 'cosine', 'cyclic'\n",
    "        warmup_epochs=5,\n",
    "        \n",
    "        # Gradient clipping\n",
    "        gradient_clipping=True,\n",
    "        max_gradient_norm=1.0,\n",
    "        \n",
    "        # Early stopping avancé\n",
    "        early_stopping_monitor='val_loss',\n",
    "        early_stopping_mode='min',\n",
    "        early_stopping_restore_best=True,\n",
    "        \n",
    "        # Architecture complexe\n",
    "        attention_heads=4,\n",
    "        use_attention=True,\n",
    "        dense_layers=[64, 32],\n",
    "        dense_activations=['relu', 'tanh']\n",
    "    )\n",
    "    \n",
    "    print(\"\\u2705 Configuration avancée appliquée\")\n",
    "else:\n",
    "    print(\"\\u2139\\ufe0f  Configuration avancée désactivée (mode simple actif)\")\n",
    "    print(\"Pour activer: config.set_advanced_mode(True) et config.set_advanced_config(...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Préparation des Données pour LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des features\n",
    "def prepare_features(df, config):\n",
    "    \"\"\"Prépare les features pour le modèle LSTM\"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Calcul des returns\n",
    "    df_features['returns'] = df_features['close'].pct_change()\n",
    "    \n",
    "    # Ajout d'indicateurs techniques\n",
    "    if 'rsi' in config.technical_indicators:\n",
    "        # RSI\n",
    "        delta = df_features['close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "        rs = gain / loss\n",
    "        df_features['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    if 'macd' in config.technical_indicators:\n",
    "        # MACD\n",
    "        ema_12 = df_features['close'].ewm(span=12).mean()\n",
    "        ema_26 = df_features['close'].ewm(span=26).mean()\n",
    "        df_features['macd'] = ema_12 - ema_26\n",
    "        df_features['macd_signal'] = df_features['macd'].ewm(span=9).mean()\n",
    "    \n",
    "    if 'bollinger_bands' in config.technical_indicators:\n",
    "        # Bollinger Bands\n",
    "        rolling_mean = df_features['close'].rolling(window=20).mean()\n",
    "        rolling_std = df_features['close'].rolling(window=20).std()\n",
    "        df_features['bb_upper'] = rolling_mean + (rolling_std * 2)\n",
    "        df_features['bb_lower'] = rolling_mean - (rolling_std * 2)\n",
    "        df_features['bb_position'] = (df_features['close'] - rolling_mean) / (2 * rolling_std)\n",
    "    \n",
    "    # Suppression des valeurs NaN\n",
    "    df_features = df_features.dropna()\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Préparation des données\n",
    "print(\"\\ud83d\\udd27 Préparation des features...\")\n",
    "df_features = prepare_features(df, config)\n",
    "print(f\"\\u2705 Features préparées: {df_features.shape[1]} colonnes\")\n",
    "print(f\"Features: {list(df_features.columns)}\")\n",
    "print(f\"Données après nettoyage: {len(df_features)} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des features\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "fig.suptitle('Features pour le Modèle LSTM', fontsize=16)\n",
    "\n",
    "# Prix et volume\n",
    "axes[0,0].plot(df_features.index, df_features['close'], linewidth=0.8)\n",
    "axes[0,0].set_title('Prix de Clôture')\n",
    "axes[0,0].set_ylabel('Prix ($)')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0,1].bar(df_features.index[::100], df_features['volume'][::100], width=0.8, alpha=0.7)\n",
    "axes[0,1].set_title('Volume')\n",
    "axes[0,1].set_ylabel('Volume')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# RSI\n",
    "if 'rsi' in df_features.columns:\n",
    "    axes[1,0].plot(df_features.index, df_features['rsi'], linewidth=0.8, color='purple')\n",
    "    axes[1,0].axhline(y=70, color='r', linestyle='--', alpha=0.7)\n",
    "    axes[1,0].axhline(y=30, color='g', linestyle='--', alpha=0.7)\n",
    "    axes[1,0].set_title('RSI')\n",
    "    axes[1,0].set_ylabel('RSI')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# MACD\n",
    "if 'macd' in df_features.columns:\n",
    "    axes[1,1].plot(df_features.index, df_features['macd'], linewidth=0.8, label='MACD')\n",
    "    axes[1,1].plot(df_features.index, df_features['macd_signal'], linewidth=0.8, label='Signal')\n",
    "    axes[1,1].set_title('MACD')\n",
    "    axes[1,1].set_ylabel('MACD')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Bollinger Bands\n",
    "if 'bb_position' in df_features.columns:\n",
    "    axes[2,0].plot(df_features.index, df_features['bb_position'], linewidth=0.8, color='orange')\n",
    "    axes[2,0].axhline(y=1, color='r', linestyle='--', alpha=0.7)\n",
    "    axes[2,0].axhline(y=-1, color='g', linestyle='--', alpha=0.7)\n",
    "    axes[2,0].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    axes[2,0].set_title('Position dans Bollinger Bands')\n",
    "    axes[2,0].set_ylabel('Position')\n",
    "    axes[2,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Returns\n",
    "axes[2,1].plot(df_features.index, df_features['returns'], linewidth=0.5, alpha=0.7)\n",
    "axes[2,1].set_title('Returns')\n",
    "axes[2,1].set_ylabel('Return')\n",
    "axes[2,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Création des Séquences Temporelles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des séquences pour LSTM\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_sequences(data, sequence_length, prediction_horizon, features, target_feature='close'):\n",
    "    \"\"\"Crée des séquences temporelles pour LSTM\"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(sequence_length, len(data) - prediction_horizon + 1):\n",
    "        # Séquence d'entrée\n",
    "        X.append(data[i-sequence_length:i][features].values)\n",
    "        \n",
    "        # Cible: prédire le prochain prix\n",
    "        target_idx = i + prediction_horizon - 1\n",
    "        y.append(data[target_idx][target_feature])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Sélection des features pour le modèle\n",
    "feature_columns = [col for col in config.features if col in df_features.columns]\n",
    "if not feature_columns:\n",
    "    feature_columns = ['close', 'volume', 'returns']\n",
    "\n",
    "print(f\"\\ud83d\\udd27 Features sélectionnées: {feature_columns}\")\n",
    "\n",
    "# Normalisation\n",
    "if config.normalize:\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = df_features.copy()\n",
    "    df_scaled[feature_columns] = scaler.fit_transform(df_features[feature_columns])\n",
    "    print(\"\\u2705 Données normalisées avec StandardScaler\")\n",
    "else:\n",
    "    df_scaled = df_features.copy()\n",
    "    scaler = None\n",
    "    print(\"\\u2139\\ufe0f  Pas de normalisation appliquée\")\n",
    "\n",
    "# Création des séquences\n",
    "print(\"\\ud83d\\udd27 Création des séquences temporelles...\")\n",
    "X, y = create_sequences(\n",
    "    df_scaled, \n",
    "    config.sequence_length, \n",
    "    config.prediction_horizon, \n",
    "    feature_columns\n",
    ")\n",
    "\n",
    "print(f\"\\u2705 Séquences créées:\")\n",
    "print(f\"  X shape: {X.shape} (échantillons, séquence_length, features)\")\n",
    "print(f\"  y shape: {y.shape} (échantillons,)\")\n",
    "print(f\"  Features par timestep: {X.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division train/validation/test\n",
    "print(\"\\ud83d\\udcca Division des données...\")\n",
    "\n",
    "# Division temporelle (80% train, 20% test)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Division train/validation du train\n",
    "val_split_idx = int(len(X_train) * (1 - config.validation_split))\n",
    "X_train, X_val = X_train[:val_split_idx], X_train[val_split_idx:]\n",
    "y_train, y_val = y_train[:val_split_idx], y_train[val_split_idx:]\n",
    "\n",
    "print(f\"\\u2705 Division terminée:\")\n",
    "print(f\"  Train: {X_train.shape[0]} échantillons\")\n",
    "print(f\"  Validation: {X_val.shape[0]} échantillons\") \n",
    "print(f\"  Test: {X_test.shape[0]} échantillons\")\n",
    "\n",
    "# Vérification des dimensions\n",
    "assert X_train.shape[1:] == X_val.shape[1:] == X_test.shape[1:], \"Dimensions incohérentes\"\n",
    "print(f\"\\u2705 Dimensions cohérentes: {X_train.shape[1:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construction du Modèle LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothèques PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Vérification du device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\ud83d\\ude80 Device utilisé: {device}\")\n",
    "\n",
    "# Définition du modèle LSTM\n",
    "class LSTMTradingModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(LSTMTradingModel, self).__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_layers = config.num_layers\n",
    "        self.bidirectional = getattr(config, 'bidirectional', False)\n",
    "        \n",
    "        # Couche LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=config.input_size,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=config.dropout if config.num_layers > 1 else 0,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        \n",
    "        # Calcul de la taille de sortie LSTM\n",
    "        lstm_output_size = config.hidden_size * (2 if self.bidirectional else 1)\n",
    "        \n",
    "        # Couches denses supplémentaires\n",
    "        dense_layers = []\n",
    "        if hasattr(config, 'dense_layers') and config.dense_layers:\n",
    "            prev_size = lstm_output_size\n",
    "            for i, dense_size in enumerate(config.dense_layers):\n",
    "                dense_layers.append(nn.Linear(prev_size, dense_size))\n",
    "                \n",
    "                # Activation\n",
    "                if hasattr(config, 'dense_activations') and i < len(config.dense_activations):\n",
    "                    if config.dense_activations[i] == 'relu':\n",
    "                        dense_layers.append(nn.ReLU())\n",
    "                    elif config.dense_activations[i] == 'tanh':\n",
    "                        dense_layers.append(nn.Tanh())\n",
    "                    elif config.dense_activations[i] == 'sigmoid':\n",
    "                        dense_layers.append(nn.Sigmoid())\n",
    "                \n",
    "                # Dropout\n",
    "                if hasattr(config, 'dropout') and config.dropout > 0:\n",
    "                    dense_layers.append(nn.Dropout(config.dropout))\n",
    "                \n",
    "                prev_size = dense_size\n",
    "            \n",
    "            # Couche de sortie\n",
    "            dense_layers.append(nn.Linear(prev_size, 1))\n",
    "        else:\n",
    "            # Sortie directe depuis LSTM\n",
    "            dense_layers.append(nn.Linear(lstm_output_size, 1))\n",
    "        \n",
    "        self.dense_layers = nn.Sequential(*dense_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, sequence_length, features)\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        # Prendre la dernière sortie temporelle\n",
    "        if self.bidirectional:\n",
    "            # Concaténer les états cachés des deux directions\n",
    "            last_hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        else:\n",
    "            last_hidden = hidden[-1]\n",
    "        \n",
    "        # Passer à travers les couches denses\n",
    "        output = self.dense_layers(last_hidden)\n",
    "        \n",
    "        return output.squeeze()\n",
    "\n",
    "# Création du modèle\n",
    "print(\"\\ud83d\\udd27 Création du modèle LSTM...\")\n",
    "config.input_size = len(feature_columns)\n",
    "model = LSTMTradingModel(config).to(device)\n",
    "\n",
    "print(f\"\\u2705 Modèle créé:\")\n",
    "print(f\"  Paramètres totaux: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Paramètres trainables: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Affichage de l'architecture\n",
    "print(\"\\ud83d\\udcca Architecture du modèle:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'optimiseur et de la fonction de perte\n",
    "print(\"\\ud83d\\udd27 Configuration de l'optimiseur et de la perte...\")\n",
    "\n",
    "# Fonction de perte\n",
    "if hasattr(config, 'loss_function'):\n",
    "    if config.loss_function == 'mse':\n",
    "        criterion = nn.MSELoss()\n",
    "    elif config.loss_function == 'mae':\n",
    "        criterion = nn.L1Loss()\n",
    "    elif config.loss_function == 'huber':\n",
    "        criterion = nn.HuberLoss()\n",
    "    else:\n",
    "        criterion = nn.MSELoss()\n",
    "else:\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "# Optimiseur\n",
    "if hasattr(config, 'optimizer'):\n",
    "    if config.optimizer == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    elif config.optimizer == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=getattr(config, 'weight_decay', 0))\n",
    "    elif config.optimizer == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=config.learning_rate)\n",
    "    elif config.optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config.learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "else:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# Planificateur de learning rate\n",
    "scheduler = None\n",
    "if hasattr(config, 'learning_rate_schedule'):\n",
    "    if config.learning_rate_schedule == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif config.learning_rate_schedule == 'cosine':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs)\n",
    "    elif config.learning_rate_schedule == 'plateau':\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)\n",
    "\n",
    "print(f\"\\u2705 Optimiseur: {type(optimizer).__name__}\")\n",
    "print(f\"\\u2705 Fonction de perte: {type(criterion).__name__}\")\n",
    "if scheduler:\n",
    "    print(f\"\\u2705 Planificateur: {type(scheduler).__name__}\")\n",
    "else:\n",
    "    print(\"\\u2139\\ufe0f  Pas de planificateur de learning rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Préparation des DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en tenseurs PyTorch\n",
    "print(\"\\ud83d\\udd27 Conversion en tenseurs PyTorch...\")\n",
    "\n",
    "# Conversion des données\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "# Création des datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Création des dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\u2705 DataLoaders créés:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "\n",
    "# Vérification d'un batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\ud83d\udcc8 Dimensions d'un batch: {sample_batch[0].shape}, {sample_batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Entraînement du Modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions d'entraînement et d'évaluation\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, gradient_clipping=None, max_gradient_norm=1.0):\n",
    "    \"\"\"Entraîne le modèle pour une époque\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if gradient_clipping:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_gradient_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Affichage de progression\n",
    "        if batch_idx % max(1, num_batches // 10) == 0:\n",
    "            print(f\"    Batch {batch_idx}/{num_batches}, Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    \"\"\"Évalue le modèle\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions.extend(output.cpu().numpy())\n",
    "            targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # Calcul des métriques supplémentaires\n",
    "    predictions = np.array(predictions)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    mae = np.mean(np.abs(predictions - targets))\n",
    "    mse = np.mean((predictions - targets) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = np.mean(np.abs((predictions - targets) / targets)) * 100\n",
    "    \n",
    "    return avg_loss, mae, mse, rmse, mape, predictions, targets\n",
    "\n",
    "# Early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.restore_best_weights:\n",
    "                self.best_weights = model.state_dict().copy()\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights and self.best_weights is not None:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "print(\"\\u2705 Fonctions d'entraînement prêtes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de l'entraînement\n",
    "print(\"\\ud83d\\ude80 D\\u00e9but de l'entraînement...\")\n",
    "\n",
    "# Historique des pertes\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_maes = []\n",
    "val_rmses = []\n",
    "val_mapes = []\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=getattr(config, 'early_stopping_patience', 10),\n",
    "    min_delta=1e-6,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Meilleur modèle\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "# Boucle d'entraînement\n",
    "for epoch in range(config.epochs):\n",
    "    print(f\"\\n\\ud83d\\udcc5\\u00c9poque {epoch + 1}/{config.epochs}\")\n",
    "    \n",
    "    # Entraînement\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device,\n",
    "        gradient_clipping=getattr(config, 'gradient_clipping', False),\n",
    "        max_gradient_norm=getattr(config, 'max_gradient_norm', 1.0)\n",
    "    )\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Évaluation\n",
    "    val_loss, val_mae, val_mse, val_rmse, val_mape, _, _ = evaluate_model(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Mise à jour du planificateur\n",
    "    if scheduler:\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "    \n",
    "    # Sauvegarde du meilleur modèle\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # Historique\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_maes.append(val_mae)\n",
    "    val_rmses.append(val_rmse)\n",
    "    val_mapes.append(val_mape)\n",
    "    \n",
    "    # Affichage\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"  \\u23f1\\ufe0f  Temps: {train_time:.2f}s\")\n",
    "    print(f\"  \\ud83d\\udcc8 Train Loss: {train_loss:.6f}\")\n",
    "    print(f\"  \\ud83d\\udcca Val Loss: {val_loss:.6f} | MAE: {val_mae:.6f} | RMSE: {val_rmse:.6f}\")\n",
    "    print(f\"  \\ud83d\\udcc9 Val MAPE: {val_mape:.2f}% | LR: {current_lr:.2e}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if early_stopping(val_loss, model):\n",
    "        print(f\"  \\u26a0\\ufe0f  Early stopping déclench\\u00e9 à l'époque {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "# Restauration du meilleur modèle\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\u2705 Meilleur mod\\u00e8le restaur\\u00e9 (Val Loss: {best_val_loss:.6f})\")\n",
    "\n",
    "print(f\"\\ud83c\\udf89 Entra\\u00eenement termin\\u00e9!\")\n",
    "print(f\"  Total \\u00e9poques: {len(train_losses)}\")\n",
    "print(f\"  Meilleure perte validation: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'entraînement\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Courbes d\\'Entra\\u00eenement du Mod\\u00e8le LSTM', fontsize=16)\n",
    "\n",
    "# Perte\n",
    "axes[0,0].plot(train_losses, label='Train Loss', linewidth=2)\n",
    "axes[0,0].plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "axes[0,0].set_title('Perte au Cours de l\\'Entra\\u00eenement')\n",
    "axes[0,0].set_xlabel('\\u00c9poque')\n",
    "axes[0,0].set_ylabel('Perte')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[0,1].plot(val_maes, label='Validation MAE', linewidth=2, color='green')\n",
    "axes[0,1].set_title('Mean Absolute Error (Validation)')\n",
    "axes[0,1].set_xlabel('\\u00c9poque')\n",
    "axes[0,1].set_ylabel('MAE')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE\n",
    "axes[1,0].plot(val_rmses, label='Validation RMSE', linewidth=2, color='red')\n",
    "axes[1,0].set_title('Root Mean Square Error (Validation)')\n",
    "axes[1,0].set_xlabel('\\u00c9poque')\n",
    "axes[1,0].set_ylabel('RMSE')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAPE\n",
    "axes[1,1].plot(val_mapes, label='Validation MAPE', linewidth=2, color='purple')\n",
    "axes[1,1].set_title('Mean Absolute Percentage Error (Validation)')\n",
    "axes[1,1].set_xlabel('\\u00c9poque')\n",
    "axes[1,1].set_ylabel('MAPE (%)')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Résumé des performances\n",
    "print(\"\\ud83d\\udcc8 R\\u00e9sum\\u00e9 des Performances d'Entra\\u00eenement:\")\n",
    "print(f\"  Meilleure \\u00e9poque: {np.argmin(val_losses) + 1}\")\n",
    "print(f\"  Meilleure validation loss: {min(val_losses):.6f}\")\n",
    "print(f\"  Meilleure validation MAE: {min(val_maes):.6f}\")\n",
    "print(f\"  Meilleure validation RMSE: {min(val_rmses):.6f}\")\n",
    "print(f\"  Meilleure validation MAPE: {min(val_mapes):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Évaluation Finale du Modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation sur l'ensemble de test\n",
    "print(\"\\ud83d\\udcca\\u00c9valuation finale sur l'ensemble de test...\")\n",
    "\n",
    "test_loss, test_mae, test_mse, test_rmse, test_mape, test_predictions, test_targets = evaluate_model(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f\"\\u2705\\u00c9valuation termin\\u00e9e:\")\n",
    "print(f\"  Test Loss: {test_loss:.6f}\")\n",
    "print(f\"  Test MAE: {test_mae:.6f}\")\n",
    "print(f\"  Test RMSE: {test_rmse:.6f}\")\n",
    "print(f\"  Test MAPE: {test_mape:.2f}%\")\n",
    "\n",
    "# Comparaison avec la validation\n",
    "print(f\"\\ud83d\\udd0d Comparaison Validation vs Test:\")\n",
    "print(f\"  Validation Loss: {min(val_losses):.6f} vs Test Loss: {test_loss:.6f}\")\n",
    "print(f\"  Validation MAE: {min(val_maes):.6f} vs Test MAE: {test_mae:.6f}\")\n",
    "print(f\"  Validation RMSE: {min(val_rmses):.6f} vs Test RMSE: {test_rmse:.6f}\")\n",
    "print(f\"  Validation MAPE: {min(val_mapes):.2f}% vs Test MAPE: {test_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des prédictions vs cibles\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Pr\\u00e9dictions vs Cibles sur l\\'Ensemble de Test', fontsize=16)\n",
    "\n",
    "# Série temporelle complète\n",
    "axes[0,0].plot(test_targets, label='Cibles', linewidth=1, alpha=0.8)\n",
    "axes[0,0].plot(test_predictions, label='Pr\\u00e9dictions', linewidth=1, alpha=0.8)\n",
    "axes[0,0].set_title('S\\u00e9rie Temporelle: Pr\\u00e9dictions vs Cibles')\n",
    "axes[0,0].set_xlabel('\\u00c9chantillon')\n",
    "axes[0,0].set_ylabel('Prix')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Zoom sur les premiers 100 échantillons\n",
    "n_samples = min(100, len(test_targets))\n",
    "axes[0,1].plot(test_targets[:n_samples], label='Cibles', linewidth=2)\n",
    "axes[0,1].plot(test_predictions[:n_samples], label='Pr\\u00e9dictions', linewidth=2)\n",
    "axes[0,1].set_title(f'Zoom sur les {n_samples} Premiers \\u00c9chantillons')\n",
    "axes[0,1].set_xlabel('\\u00c9chantillon')\n",
    "axes[0,1].set_ylabel('Prix')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot\n",
    "axes[1,0].scatter(test_targets, test_predictions, alpha=0.6, s=20)\n",
    "min_val = min(test_targets.min(), test_predictions.min())\n",
    "max_val = max(test_targets.max(), test_predictions.max())\n",
    "axes[1,0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "axes[1,0].set_title('Scatter Plot: Pr\\u00e9dictions vs Cibles')\n",
    "axes[1,0].set_xlabel('Cibles')\n",
    "axes[1,0].set_ylabel('Pr\\u00e9dictions')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution des erreurs\n",
    "errors = test_predictions - test_targets\n",
    "axes[1,1].hist(errors, bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[1,1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1,1].set_title('Distribution des Erreurs de Pr\\u00e9diction')\n",
    "axes[1,1].set_xlabel('Erreur')\n",
    "axes[1,1].set_ylabel('Fr\\u00e9quence')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques des erreurs\n",
    "print(\"\\ud83d\\udcc8 Statistiques des Erreurs:\")\n",
    "print(f\"  Moyenne: {errors.mean():.6f}\")\n",
    "print(f\"  \\u00c9cart-type: {errors.std():.6f}\")\n",
    "print(f\"  Min: {errors.min():.6f}\")\n",
    "print(f\"  Max: {errors.max():.6f}\")\n",
    "print(f\"  Erreurs dans \\u00b11\\u00b5: {np.sum(np.abs(errors) <= errors.std()):.1f}%\")\n",
    "print(f\"  Erreurs dans \\u00b12\\u00b5: {np.sum(np.abs(errors) <= 2*errors.std()):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des performances par plage de prix\n",
    "def analyze_performance_by_range(targets, predictions, num_bins=5):\n",
    "    \"\"\"Analyse les performances par plage de prix\"\"\"\n",
    "    # Création des bins\n",
    "    bins = np.linspace(targets.min(), targets.max(), num_bins + 1)\n",
    "    bin_labels = [f'[{bins[i]:.1f}, {bins[i+1]:.1f})' for i in range(num_bins)]\n",
    "    \n",
    "    # Assignation aux bins\n",
    "    bin_indices = np.digitize(targets, bins) - 1\n",
    "    bin_indices = np.clip(bin_indices, 0, num_bins - 1)\n",
    "    \n",
    "    # Calcul des métriques par bin\n",
    "    results = []\n",
    "    for i in range(num_bins):\n",
    "        mask = bin_indices == i\n",
    "        if np.sum(mask) > 0:\n",
    "            bin_targets = targets[mask]\n",
    "            bin_predictions = predictions[mask]\n",
    "            \n",
    "            mae = np.mean(np.abs(bin_predictions - bin_targets))\n",
    "            rmse = np.sqrt(np.mean((bin_predictions - bin_targets) ** 2))\n",
    "            mape = np.mean(np.abs((bin_predictions - bin_targets) / bin_targets)) * 100\n",
    "            \n",
    "            results.append({\n",
    "                'bin': bin_labels[i],\n",
    "                'count': np.sum(mask),\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'mape': mape\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyse par plage de prix\n",
    "print(\"\\ud83d\\udcca Analyse des Performances par Plage de Prix:\")\n",
    "performance_analysis = analyze_performance_by_range(test_targets, test_predictions)\n",
    "\n",
    "for result in performance_analysis:\n",
    "    print(f\"  {result['bin']}: n={result['count']}, MAE={result['mae']:.4f}, RMSE={result['rmse']:.4f}, MAPE={result['mape']:.2f}%\")\n",
    "\n",
    "# Analyse temporelle (si on a les timestamps)\n",
    "if len(test_targets) > 100:\n",
    "    # Analyse par périodes\n",
    "    window_size = len(test_targets) // 4\n",
    "    print(f\"\\ud83d\\udcc8 Analyse Temporelle (fen\\u00eatre: {window_size} \\u00e9chantillons):\")\n",
    "    \n",
    "    for i in range(4):\n",
    "        start_idx = i * window_size\n",
    "        end_idx = min((i + 1) * window_size, len(test_targets))\n",
    "        \n",
    "        period_targets = test_targets[start_idx:end_idx]\n",
    "        period_predictions = test_predictions[start_idx:end_idx]\n",
    "        \n",
    "        period_mae = np.mean(np.abs(period_predictions - period_targets))\n",
    "        period_rmse = np.sqrt(np.mean((period_predictions - period_targets) ** 2))\n",
    "        period_mape = np.mean(np.abs((period_predictions - period_targets) / period_targets)) * 100\n",
    "        \n",
    "        print(f\"  P\\u00e9riode {i+1}: MAE={period_mae:.4f}, RMSE={period_rmse:.4f}, MAPE={period_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sauvegarde du Modèle et Préparation pour PPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du modèle entraîné\n",
    "import torch\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Création du dossier de sauvegarde\n",
    "save_dir = 'trained_models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Génération du nom de fichier avec timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'{save_dir}/lstm_model_{timestamp}.pth'\n",
    "config_filename = f'{save_dir}/lstm_config_{timestamp}.pkl'\n",
    "scaler_filename = f'{save_dir}/lstm_scaler_{timestamp}.pkl'\n",
    "\n",
    "# Sauvegarde du modèle\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config,\n",
    "    'model_class': 'LSTMTradingModel',\n",
    "    'input_size': config.input_size,\n",
    "    'hidden_size': config.hidden_size,\n",
    "    'num_layers': config.num_layers,\n",
    "    'sequence_length': config.sequence_length,\n",
    "    'features': feature_columns,\n",
    "    'performance': {\n",
    "        'test_loss': test_loss,\n",
    "        'test_mae': test_mae,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mape': test_mape\n",
    "    },\n",
    "    'timestamp': timestamp\n",
    "}, model_filename)\n",
    "\n",
    "# Sauvegarde du scaler\n",
    "if scaler is not None:\n",
    "    with open(scaler_filename, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"\\u2705 Mod\\u00e8le sauvegard\\u00e9:\")\n",
    "print(f\"  Fichier mod\\u00e8le: {model_filename}\")\n",
    "print(f\"  Fichier scaler: {scaler_filename}\")\n",
    "print(f\"  Performance: MAE={test_mae:.6f}, RMSE={test_rmse:.6f}, MAPE={test_mape:.2f}%\")\n",
    "\n",
    "# Création d'un résumé d'entra\\u00eenement\n",
    "training_summary = {\n",
    "    'model_path': model_filename,\n",
    "    'scaler_path': scaler_filename if scaler else None,\n",
    "    'config': {\n",
    "        'sequence_length': config.sequence_length,\n",
    "        'prediction_horizon': config.prediction_horizon,\n",
    "        'hidden_size': config.hidden_size,\n",
    "        'num_layers': config.num_layers,\n",
    "        'features': feature_columns,\n",
    "        'bidirectional': getattr(config, 'bidirectional', False),\n",
    "        'dropout': config.dropout\n",
    "    },\n",
    "    'performance': {\n",
    "        'test_mae': test_mae,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mape': test_mape,\n",
    "        'best_val_loss': best_val_loss\n",
    "    },\n",
    "    'data_info': {\n",
    "        'ticker': TICKER,\n",
    "        'date_range': f\"{START_DATE} to {END_DATE}\",\n",
    "        'train_samples': len(X_train),\n",
    "        'val_samples': len(X_val),\n",
    "        'test_samples': len(X_test)\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_filename = f'{save_dir}/training_summary_{timestamp}.json'\n",
    "import json\n",
    "with open(summary_filename, 'w') as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\u2705 R\\u00e9sum\\u00e9 d'entra\\u00eenement sauvegard\\u00e9: {summary_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe pour utiliser le modèle LSTM avec PPO\n",
    "class LSTMPredictor:\n",
    "    \"\"\"Classe pour charger et utiliser le mod\\u00e8le LSTM entra\\u00een\\u00e9\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, scaler_path=None, device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialise le pr\\u00e9dicteur LSTM\n",
    "        \n",
    "        Args:\n",
    "            model_path: Chemin vers le fichier mod\\u00e8le .pth\n",
    "            scaler_path: Chemin vers le fichier scaler .pkl (optionnel)\n",
    "            device: Device PyTorch ('cpu' ou 'cuda')\n",
    "        \"\"\"\n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        # Chargement du checkpoint\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        \n",
    "        # R\\u00e9cup\\u00e9ration des param\\u00e8tres\n",
    "        self.config = checkpoint['config']\n",
    "        self.sequence_length = checkpoint['sequence_length']\n",
    "        self.features = checkpoint['features']\n",
    "        self.input_size = checkpoint['input_size']\n",
    "        \n",
    "        # Chargement du scaler\n",
    "        if scaler_path and os.path.exists(scaler_path):\n",
    "            with open(scaler_path, 'rb') as f:\n",
    "                self.scaler = pickle.load(f)\n",
    "        else:\n",
    "            self.scaler = None\n",
    "        \n",
    "        # Cr\\u00e9ation et chargement du mod\\u00e8le\n",
    "        self.model = LSTMTradingModel(self.config).to(self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Buffer pour stocker l'historique\n",
    "        self.history_buffer = []\n",
    "        \n",
    "        print(f\"\\u2705 Mod\\u00e8le LSTM charg\\u00e9:\")\n",
    "        print(f\"  Performance: MAE={checkpoint['performance']['test_mae']:.6f}\")\n",
    "        print(f\"  Features: {self.features}\")\n",
    "        print(f\"  Sequence length: {self.sequence_length}\")\n",
    "    \n",
    "    def preprocess_data(self, data):\n",
    "        \"\"\"\n",
    "        Pr\\u00e9pare les donn\\u00e9es pour la pr\\u00e9diction\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame avec les donn\\u00e9es de march\\u00e9\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Donn\\u00e9es pr\\u00e9par\\u00e9es\n",
    "        \"\"\"\n",
    "        # Calcul des features n\\u00e9cessaires\n",
    "        df_features = data.copy()\n",
    "        \n",
    "        if 'returns' in self.features and 'returns' not in df_features.columns:\n",
    "            df_features['returns'] = df_features['close'].pct_change()\n",
    "        \n",
    "        # RSI\n",
    "        if 'rsi' in self.features:\n",
    "            delta = df_features['close'].diff()\n",
    "            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "            rs = gain / loss\n",
    "            df_features['rsi'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # MACD\n",
    "        if 'macd' in self.features:\n",
    "            ema_12 = df_features['close'].ewm(span=12).mean()\n",
    "            ema_26 = df_features['close'].ewm(span=26).mean()\n",
    "            df_features['macd'] = ema_12 - ema_26\n",
    "            df_features['macd_signal'] = df_features['macd'].ewm(span=9).mean()\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        if 'bb_position' in self.features:\n",
    "            rolling_mean = df_features['close'].rolling(window=20).mean()\n",
    "            rolling_std = df_features['close'].rolling(window=20).std()\n",
    "            df_features['bb_position'] = (df_features['close'] - rolling_mean) / (2 * rolling_std)\n",
    "        \n",
    "        # S\\u00e9lection des features\n",
    "        feature_data = df_features[self.features].dropna()\n",
    "        \n",
    "        # Normalisation\n",
    "        if self.scaler is not None:\n",
    "            feature_data = self.scaler.transform(feature_data)\n",
    "        \n",
    "        return feature_data\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Effectue une pr\\u00e9diction sur les donn\\u00e9es\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame avec les donn\\u00e9es de march\\u00e9 (doit contenir au moins sequence_length lignes)\n",
    "        \n",
    "        Returns:\n",
    "            float: Pr\\u00e9diction du prix futur\n",
    "        \"\"\"\n",
    "        if len(data) < self.sequence_length:\n",
    "            raise ValueError(f\"Donn\\u00e9es insuffisantes: {len(data)} < {self.sequence_length}\")\n",
    "        \n",
    "        # Pr\\u00e9paration des donn\\u00e9es\n",
    "        features = self.preprocess_data(data)\n",
    "        \n",
    "        # Pr\\u00e9paration de la s\\u00e9quence\n",
    "        sequence = features[-self.sequence_length:]\n",
    "        sequence_tensor = torch.FloatTensor(sequence).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Pr\\u00e9diction\n",
    "        with torch.no_grad():\n",
    "            prediction = self.model(sequence_tensor)\n",
    "        \n",
    "        return prediction.item()\n",
    "    \n",
    "    def predict_sequence(self, data_sequence):\n",
    "        \"\"\"\n",
    "        Effectue des pr\\u00e9dictions sur une s\\u00e9quence de donn\\u00e9es\n",
    "        \n",
    "        Args:\n",
    "            data_sequence: Liste de DataFrames avec les donn\\u00e9es de march\\u00e9\n",
    "        \n",
    "        Returns:\n",
    "            list: Liste des pr\\u00e9dictions\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for data in data_sequence:\n",
    "            try:\n",
    "                pred = self.predict(data)\n",
    "                predictions.append(pred)\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de la pr\\u00e9diction: {e}\")\n",
    "                predictions.append(None)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "print(\"\\u2705 Classe LSTMPredictor cr\\u00e9\\u00e9e - Pr\\u00eate pour l'int\\u00e9gration PPO!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du pr\\u00e9dicteur avec le mod\\u00e8le entra\\u00een\\u00e9\n",
    "print(\"\\ud83d\\ude80 Test du pr\\u00e9dicteur LSTM...\")\n",
    "\n",
    "# Cr\\u00e9ation du pr\\u00e9dicteur\n",
    "predictor = LSTMPredictor(model_filename, scaler_filename if scaler else None, device=str(device))\n",
    "\n",
    "# Test sur un \\u00e9chantillon\n",
    "test_data = df_features.tail(100)  # 100 derni\\u00e8res lignes\n",
    "prediction = predictor.predict(test_data)\n",
    "\n",
    "print(f\"\\u2705 Test de pr\\u00e9diction r\\u00e9ussi:\")\n",
    "print(f\"  Derni\\u00e8re valeur r\\u00e9elle: {test_data['close'].iloc[-1]:.4f}\")\n",
    "print(f\"  Pr\\u00e9diction: {prediction:.4f}\")\n",
    "print(f\"  Diff\\u00e9rence: {prediction - test_data['close'].iloc[-1]:.4f}\")\n",
    "\n",
    "# Test sur plusieurs p\\u00e9riodes\n",
    "print(\"\\ud83d\\udcca Test sur plusieurs p\\u00e9riodes...\")\n",
    "test_periods = []\n",
    "for i in range(5):\n",
    "    start_idx = -150 - i*10\n",
    "    end_idx = -50 - i*10\n",
    "    test_periods.append(df_features.iloc[start_idx:end_idx])\n",
    "\n",
    "predictions = predictor.predict_sequence(test_periods)\n",
    "actual_values = [period['close'].iloc[-1] for period in test_periods]\n",
    "\n",
    "print(\"Comparaison pr\\u00e9dictions vs valeurs r\\u00e9elles:\")\n",
    "for i, (pred, actual) in enumerate(zip(predictions, actual_values)):\n",
    "    error = abs(pred - actual) if pred is not None else float('inf')\n",
    "    print(f\"  P\\u00e9riode {i+1}: Pr\\u00e9diction={pred:.4f}, R\\u00e9el={actual:.4f}, Erreur={error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Intégration avec PPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe d'int\\u00e9gration PPO-LSTM\n",
    "class PPOIntegration:\n",
    "    \"\"\"Classe pour int\\u00e9grer les pr\\u00e9dictions LSTM dans PPO\"\"\"\n",
    "    \n",
    "    def __init__(self, lstm_predictor, confidence_threshold=0.1):\n",
    "        \"\"\"\n",
    "        Initialise l'int\\u00e9gration\n",
    "        \n",
    "        Args:\n",
    "            lstm_predictor: Instance de LSTMPredictor\n",
    "            confidence_threshold: Seuil de confiance pour les pr\\u00e9dictions\n",
    "        \"\"\"\n",
    "        self.predictor = lstm_predictor\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.prediction_history = []\n",
    "        self.prediction_errors = []\n",
    "    \n",
    "    def get_lstm_signal(self, market_data):\n",
    "        \"\"\"\n",
    "        G\\u00e9n\\u00e8re un signal de trading bas\\u00e9 sur la pr\\u00e9diction LSTM\n",
    "        \n",
    "        Args:\n",
    "            market_data: DataFrame avec les donn\\u00e9es de march\\u00e9\n",
    "        \n",
    "        Returns:\n",
    "            dict: Signal avec direction, force et confiance\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Pr\\u00e9diction\n",
    "            prediction = self.predictor.predict(market_data)\n",
    "            current_price = market_data['close'].iloc[-1]\n",
    "            \n",
    "            # Calcul du signal\n",
    "            price_change = (prediction - current_price) / current_price\n",
    "            \n",
    "            # Direction du signal\n",
    "            if price_change > self.confidence_threshold:\n",
    "                direction = 'BUY'\n",
    "                strength = min(abs(price_change) * 10, 1.0)  # Force normalis\\u00e9e\n",
    "            elif price_change < -self.confidence_threshold:\n",
    "                direction = 'SELL'\n",
    "                strength = min(abs(price_change) * 10, 1.0)\n",
    "            else:\n",
    "                direction = 'HOLD'\n",
    "                strength = 0.0\n",
    "            \n",
    "            # Confiance bas\\u00e9e sur l'historique des erreurs\n",
    "            confidence = self._calculate_confidence()\n",
    "            \n",
    "            signal = {\n",
    "                'direction': direction,\n",
    "                'strength': strength,\n",
    "                'confidence': confidence,\n",
    "                'predicted_price': prediction,\n",
    "                'current_price': current_price,\n",
    "                'expected_return': price_change\n",
    "            }\n",
    "            \n",
    "            # Historique\n",
    "            self.prediction_history.append(prediction)\n",
    "            \n",
    "            return signal\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du calcul du signal: {e}\")\n",
    "            return {\n",
    "                'direction': 'HOLD',\n",
    "                'strength': 0.0,\n",
    "                'confidence': 0.0,\n",
    "                'predicted_price': current_price,\n",
    "                'current_price': current_price,\n",
    "                'expected_return': 0.0\n",
    "            }\n",
    "    \n",
    "    def _calculate_confidence(self):\n",
    "        \"\"\"Calcule la confiance bas\\u00e9e sur l'historique des performances\"\"\"\n",
    "        # Pour l'instant, retourne une confiance fixe\n",
    "        # Dans une impl\\u00e9mentation r\\u00e9elle, on utiliserait l'historique des erreurs\n",
    "        return 0.8\n",
    "    \n",
    "    def get_state_features(self, market_data):\n",
    "        \"\"\"\n",
    "        G\\u00e9n\\u00e8re des features suppl\\u00e9mentaires pour l'\\u00e9tat PPO\n",
    "        \n",
    "        Args:\n",
    "            market_data: DataFrame avec les donn\\u00e9es de march\\u00e9\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Features suppl\\u00e9mentaires pour l'\\u00e9tat PPO\n",
    "        \"\"\"\n",
    "        signal = self.get_lstm_signal(market_data)\n",
    "        \n",
    "        # Features pour PPO\n",
    "        features = np.array([\n",
    "            1.0 if signal['direction'] == 'BUY' else 0.0,\n",
    "            1.0 if signal['direction'] == 'SELL' else 0.0,\n",
    "            signal['strength'],\n",
    "            signal['confidence'],\n",
    "            signal['expected_return'],\n",
    "            np.clip(signal['predicted_price'] / signal['current_price'] - 1.0, -1.0, 1.0)\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "\n",
    "print(\"\\u2705 Classe d'int\\u00e9gration PPO-LSTM cr\\u00e9\\u00e9e!\")\n",
    "print(\"\\ud83d\\udcc8 Le mod\\u00e8le LSTM est maintenant pr\\u00eat \\u00e0 \\u00eatre utilis\\u00e9 avec PPO!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "\\u2705 **Notebook d'Entra\\u00eenement LSTM Termin\\u00e9!**\n",
    "\n",
    "Ce notebook a permis de:\n",
    "1. \\u2713 Charger et pr\\u00e9parer les donn\\u00e9es de trading\n",
    "2. \\u2713 Configurer le mod\\u00e8le LSTM avec tous les param\\u00e8tres disponibles\n",
    "3. \\u2713 Entra\\u00eener le mod\\u00e8le avec monitoring des performances\n",
    "4. \\u2713 \\u00c9valuer le mod\\u00e8le sur l'ensemble de test\n",
    "5. \\u2713 Sauvegarder le mod\\u00e8le pour une utilisation future\n",
    "6. \\u2713 Cr\\u00e9er une classe d'int\\u00e9gration pour PPO\n",
    "\n",
    "\\ud83d\\udd17 **Prochaines \\u00e9tapes:**\n",
    "- Utiliser le mod\\u00e8le sauvegard\\u00e9 dans `train_minute_model_lstm.py`\n",
    "- Charger le pr\\u00e9dicteur avec: `LSTMPredictor(model_path, scaler_path)`\n",
    "- Int\\u00e9grer les pr\\u00e9dictions dans l'environnement PPO via `PPOIntegration`\n",
    "\n",
    "\\ud83d\\udcc1 **Fichiers cr\\u00e9\\u00e9s:**\n",
    "- Mod\\u00e8le: `{model_filename}`\n",
    "- Scaler: `{scaler_filename}`\n",
    "- R\\u00e9sum\\u00e9: `{summary_filename}`\n",
    "\n",
    "Le mod\\u00e8le LSTM est maintenant pr\\u00eat pour l'entra\\u00eenement PPO! \\ud83d\\ude80\"\n   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code pour charger et utiliser le mod\\u00e8le dans PPO\n",
    "print(\"\\ud83d\\udcc1 Exemple d'utilisation dans PPO:\")\n",
    "print(\"\\n\")\n",
    "print(\"# Dans train_minute_model_lstm.py:\")\n",
    "print(\"from lstm_predictor import LSTMPredictor, PPOIntegration\")\n",
    "print(\"\")\n",
    "print(\"# Chargement du mod\\u00e8le\")\n",
    "print(\"lstm_predictor = LSTMPredictor('trained_models/lstm_model_*.pth', 'trained_models/lstm_scaler_*.pkl')\")\n",
    "print(\"ppo_integration = PPOIntegration(lstm_predictor)\")\n",
    "print(\"\")\n",
    "print(\"# Dans l'environnement PPO:\")\n",
    "print(\"lstm_features = ppo_integration.get_state_features(market_data)\")\n",
    "print(\"full_state = np.concatenate([base_state, lstm_features])\")\n",
    "print(\"\")\n",
    "print(\"# Le signal LSTM peut aussi \\u00eatre utilis\\u00e9 directement:\")\n",
    "print(\"lstm_signal = ppo_integration.get_lstm_signal(market_data)\")\n",
    "print(\"if lstm_signal['direction'] == 'BUY' and lstm_signal['confidence'] > 0.7:\")\n",
    "print(\"    # Renforcer l'action d'achat\")\n",
    "print(\"    action_strength *= 1.5\")\n",
    "print(\"\\ud83d\\ude80 Bonne chance avec l'entra\\u00eenement PPO!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook d'Entra√Ænement LSTM pour Trading Automatique\n",
    "\n",
    "Ce notebook permet d'entra√Æner un mod√®le LSTM avec tous les param√®tres configurables pour l'utilisation avec PPO.\n",
    "\n",
    "## Objectifs:\n",
    "- Charger et visualiser les donn√©es de trading\n",
    "- Configurer le mod√®le LSTM avec tous les param√®tres disponibles\n",
    "- Entra√Æner le mod√®le avec suivi des performances\n",
    "- Tester le mod√®le et pr√©parer l'int√©gration PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible: False\n"
     ]
    }
   ],
   "source": [
    "# Import des biblioth√®ques n√©cessaires\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration pour GPU\n",
    "import torch\n",
    "print(f\"GPU disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.cuda.set_device(0)\n",
    "\n",
    "# Ajout du r√©pertoire courant au path\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration et Chargement des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules charg√©s avec succ√®s!\n"
     ]
    }
   ],
   "source": [
    "# Configuration des variables d'environnement\n",
    "os.environ['ALPACA_API_KEY'] = 'PKWDXUMR64LL03LXOZFN'\n",
    "os.environ['ALPACA_API_SECRET'] = 'RLtrY3Idh1vQqNizfUJRoJprRJQ9uijfIoLLW4XA'\n",
    "\n",
    "# Import des modules du projet\n",
    "from data_loader import DataLoader\n",
    "from lstm_config import LSTMConfigurator\n",
    "from lstm_model import LSTMModel\n",
    "\n",
    "\n",
    "print(\"Modules charg√©s avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des donn√©es pour AAPL\n",
      "P√©riode: 2023-01-01 √† 2024-01-01\n",
      "Longueur des s√©quences: 60\n",
      "Horizon de pr√©diction: 5 jours\n"
     ]
    }
   ],
   "source": [
    "# Configuration du chargement des donn√©es\n",
    "config = LSTMConfigurator()\n",
    "\n",
    "# Param√®tres de donn√©es\n",
    "TICKER = 'AAPL'\n",
    "START_DATE = '2023-01-01'\n",
    "END_DATE = '2024-01-01'\n",
    "SEQUENCE_LENGTH = 60\n",
    "PREDICTION_HORIZON = 5\n",
    "\n",
    "print(f\"Chargement des donn√©es pour {TICKER}\")\n",
    "print(f\"P√©riode: {START_DATE} √† {END_DATE}\")\n",
    "print(f\"Longueur des s√©quences: {SEQUENCE_LENGTH}\")\n",
    "print(f\"Horizon de pr√©diction: {PREDICTION_HORIZON} jours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors du chargement des donn√©es: 'DataLoader' object has no attribute 'load_stock_data'\n",
      "Utilisation de donn√©es simul√©es pour la d√©monstration\n",
      "Donn√©es simul√©es cr√©√©es: 525601 lignes\n"
     ]
    }
   ],
   "source": [
    "# Chargement des donn√©es\n",
    "data_loader = DataLoader()\n",
    "\n",
    "try:\n",
    "    # Chargement des donn√©es historiques\n",
    "    data = data_loader.load_stock_data(\n",
    "        ticker=TICKER,\n",
    "        start_date=START_DATE,\n",
    "        end_date=END_DATE,\n",
    "        use_alpaca=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Donn√©es charg√©es: {len(data)} lignes\")\n",
    "    print(f\"Colonnes disponibles: {list(data.columns)}\")\n",
    "    print(f\"Plage de dates: {data.index[0]} √† {data.index[-1]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement des donn√©es: {e}\")\n",
    "    print(\"Utilisation de donn√©es simul√©es pour la d√©monstration\")\n",
    "    \n",
    "    # Cr√©ation de donn√©es simul√©es si le chargement √©choue\n",
    "    dates = pd.date_range(start=START_DATE, end=END_DATE, freq='1min')\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # G√©n√©ration de donn√©es de prix r√©alistes\n",
    "    initial_price = 150.0\n",
    "    returns = np.random.normal(0.0001, 0.02, len(dates))\n",
    "    prices = initial_price * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'close': prices,\n",
    "        'open': prices * (1 + np.random.normal(0, 0.001, len(dates))),\n",
    "        'high': prices * (1 + np.abs(np.random.normal(0, 0.002, len(dates)))),\n",
    "        'low': prices * (1 - np.abs(np.random.normal(0, 0.002, len(dates)))),\n",
    "        'volume': np.random.randint(100000, 1000000, len(dates))\n",
    "    }, index=dates)\n",
    "    \n",
    "    print(f\"Donn√©es simul√©es cr√©√©es: {len(data)} lignes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualisation des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des prix\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(data.index, data['close'], label='Prix de cl√¥ture', alpha=0.8)\n",
    "plt.title(f'Prix de cl√¥ture de {TICKER}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Prix ($)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(data.index, data['volume'], label='Volume', alpha=0.8, color='orange')\n",
    "plt.title(f'Volume de trading de {TICKER}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volume')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques descriptives\n",
    "print(\"Statistiques des prix:\")\n",
    "print(data[['open', 'high', 'low', 'close', 'volume']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des rendements\n",
    "data['returns'] = data['close'].pct_change()\n",
    "data['log_returns'] = np.log(data['close'] / data['close'].shift(1))\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(data.index[1:], data['returns'][1:], alpha=0.7)\n",
    "plt.title('Rendements simples')\n",
    "plt.ylabel('Rendement')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(data.index[1:], data['log_returns'][1:], alpha=0.7, color='green')\n",
    "plt.title('Rendements logarithmiques')\n",
    "plt.ylabel('Rendement log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.hist(data['log_returns'][1:], bins=50, alpha=0.7, density=True)\n",
    "plt.title('Distribution des rendements logarithmiques')\n",
    "plt.xlabel('Rendement log')\n",
    "plt.ylabel('Densit√©')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Volatilit√© annualis√©e: {data['log_returns'].std() * np.sqrt(252):.2%}\")\n",
    "print(f\"Rendement moyen journalier: {data['log_returns'].mean():.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration du Mod√®le LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du mod√®le LSTM\n",
    "config = LSTMConfigurator()\n",
    "\n",
    "# Mode de configuration: 'simple' ou 'advanced'\n",
    "CONFIG_MODE = 'advanced'  # Changez √† 'simple' pour une configuration simplifi√©e\n",
    "\n",
    "if CONFIG_MODE == 'simple':\n",
    "    print(\"Mode de configuration: SIMPLE\")\n",
    "    # Configuration simple avec param√®tres de base\n",
    "    config.set_parameters({\n",
    "        'sequence_length': SEQUENCE_LENGTH,\n",
    "        'prediction_horizon': PREDICTION_HORIZON,\n",
    "        'hidden_size': 128,\n",
    "        'num_layers': 2,\n",
    "        'dropout': 0.2,\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 32,\n",
    "        'epochs': 50\n",
    "    })\n",
    "else:\n",
    "    print(\"Mode de configuration: ADVANCED\")\n",
    "    # Configuration avanc√©e avec tous les param√®tres\n",
    "    config.set_parameters({\n",
    "        # Param√®tres de base\n",
    "        'sequence_length': SEQUENCE_LENGTH,\n",
    "        'prediction_horizon': PREDICTION_HORIZON,\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 3,\n",
    "        'dropout': 0.3,\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 64,\n",
    "        'epochs': 100,\n",
    "        \n",
    "        # Param√®tres avanc√©s LSTM\n",
    "        'bidirectional': True,  # LSTM bidirectionnel\n",
    "        'batch_first': True,\n",
    "        'bias': True,\n",
    "        \n",
    "        # Normalisation et r√©gularisation\n",
    "        'batch_normalization': True,\n",
    "        'layer_normalization': False,\n",
    "        'dropout_type': 'standard',  # 'standard', 'variational', 'monte_carlo'\n",
    "        'recurrent_dropout': 0.2,\n",
    "        \n",
    "        # Fonctions d'activation\n",
    "        'activation_function': 'tanh',  # 'tanh', 'relu', 'leaky_relu', 'gelu'\n",
    "        'recurrent_activation': 'sigmoid',\n",
    "        \n",
    "        # Optimiseur\n",
    "        'optimizer': 'adam',  # 'adam', 'adamw', 'rmsprop', 'sgd'\n",
    "        'weight_decay': 0.0001,\n",
    "        'gradient_clipping': 1.0,\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        'use_lr_scheduler': True,\n",
    "        'lr_scheduler_type': 'reduce_on_plateau',  # 'step', 'exponential', 'cosine', 'reduce_on_plateau'\n",
    "        'lr_scheduler_patience': 10,\n",
    "        'lr_scheduler_factor': 0.5,\n",
    "        \n",
    "        # Early stopping\n",
    "        'use_early_stopping': True,\n",
    "        'early_stopping_patience': 15,\n",
    "        'early_stopping_min_delta': 0.0001,\n",
    "        \n",
    "        # Architecture du mod√®le\n",
    "        'use_attention': False,  # Attention mechanism\n",
    "        'attention_heads': 4,\n",
    "        'attention_dropout': 0.1,\n",
    "        \n",
    "        # Pr√©traitement\n",
    "        'normalize_data': True,\n",
    "        'standardization_method': 'standard',  # 'standard', 'minmax', 'robust'\n",
    "        'feature_engineering': True,\n",
    "        \n",
    "        # Training\n",
    "        'validation_split': 0.2,\n",
    "        'test_split': 0.1,\n",
    "        'shuffle_training': True,\n",
    "        'mixed_precision': True,  # Utilisation de float16 pour acc√©l√©rer l'entra√Ænement\n",
    "        \n",
    "        # Sauvegarde et logging\n",
    "        'save_model_frequency': 10,\n",
    "        'log_training_metrics': True,\n",
    "        'verbose_training': True\n",
    "    })\n",
    "\n",
    "print(\"Configuration du mod√®le:\")\n",
    "config.display_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de la configuration d√©taill√©e\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFIGURATION D√âTAILL√âE DU MOD√àLE LSTM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "params = config.get_parameters()\n",
    "for category, values in params.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    if isinstance(values, dict):\n",
    "        for key, value in values.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"  {values}\")\n",
    "\n",
    "print(f\"\\nNombre total de param√®tres configurables: {len([v for subdict in params.values() for v in (subdict.values() if isinstance(subdict, dict) else [subdict])])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pr√©paration des Donn√©es pour le Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©paration des donn√©es pour le mod√®le LSTM\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "def prepare_lstm_data(data, sequence_length, prediction_horizon, standardization_method='standard'):\n",
    "    \"\"\"\n",
    "    Pr√©pare les donn√©es pour l'entra√Ænement LSTM\n",
    "    \"\"\"\n",
    "    # S√©lection des features\n",
    "    features = ['close', 'open', 'high', 'low', 'volume']\n",
    "    if 'log_returns' in data.columns:\n",
    "        features.append('log_returns')\n",
    "    \n",
    "    # Calcul des indicateurs techniques suppl√©mentaires\n",
    "    data['sma_10'] = data['close'].rolling(window=10).mean()\n",
    "    data['sma_20'] = data['close'].rolling(window=20).mean()\n",
    "    data['rsi'] = calculate_rsi(data['close'], 14)\n",
    "    data['volatility'] = data['log_returns'].rolling(window=20).std()\n",
    "    \n",
    "    # Ajout des nouvelles features\n",
    "    features.extend(['sma_10', 'sma_20', 'rsi', 'volatility'])\n",
    "    \n",
    "    # Suppression des valeurs NaN\n",
    "    data_clean = data[features].dropna()\n",
    "    \n",
    "    # Normalisation\n",
    "    if standardization_method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif standardization_method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif standardization_method == 'robust':\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    scaled_data = scaler.fit_transform(data_clean)\n",
    "    \n",
    "    # Cr√©ation des s√©quences\n",
    "    X, y = [], []\n",
    "    for i in range(sequence_length, len(scaled_data) - prediction_horizon):\n",
    "        X.append(scaled_data[i-sequence_length:i])\n",
    "        # Pr√©dire le prix de cl√¥ture futur\n",
    "        y.append(data_clean['close'].iloc[i + prediction_horizon])\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y, scaler, data_clean.index[sequence_length:len(scaled_data) - prediction_horizon]\n",
    "\n",
    "def calculate_rsi(prices, window=14):\n",
    "    \"\"\"Calcul du RSI (Relative Strength Index)\"\"\"\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "# Pr√©paration des donn√©es\n",
    "print(\"Pr√©paration des donn√©es pour le mod√®le LSTM...\")\n",
    "X, y, scaler, dates = prepare_lstm_data(\n",
    "    data, \n",
    "    sequence_length=SEQUENCE_LENGTH, \n",
    "    prediction_horizon=PREDICTION_HORIZON,\n",
    "    standardization_method=params.get('standardization_method', 'standard')\n",
    ")\n",
    "\n",
    "print(f\"Donn√©es pr√©par√©es:\")\n",
    "print(f\"  X shape: {X.shape}\")\n",
    "print(f\"  y shape: {y.shape}\")\n",
    "print(f\"  Features: {X.shape[2]}\")\n",
    "print(f\"  P√©riode: {dates[0]} √† {dates[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division des donn√©es\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split train/validation/test\n",
    "validation_split = params.get('validation_split', 0.2)\n",
    "test_split = params.get('test_split', 0.1)\n",
    "\n",
    "# D'abord s√©parer le test set\n",
    "X_temp, X_test, y_temp, y_test, dates_temp, dates_test = train_test_split(\n",
    "    X, y, dates, test_size=test_split, shuffle=False\n",
    ")\n",
    "\n",
    "# Puis s√©parer train et validation\n",
    "X_train, X_val, y_train, y_val, dates_train, dates_val = train_test_split(\n",
    "    X_temp, y_temp, dates_temp, test_size=validation_split/(1-test_split), shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Division des donn√©es:\")\n",
    "print(f\"  Train: {X_train.shape[0]} √©chantillons ({dates_train[0]} √† {dates_train[-1]})\")\n",
    "print(f\"  Validation: {X_val.shape[0]} √©chantillons ({dates_val[0]} √† {dates_val[-1]})\")\n",
    "print(f\"  Test: {X_test.shape[0]} √©chantillons ({dates_test[0]} √† {dates_test[-1]})\")\n",
    "\n",
    "# V√©rification des dimensions\n",
    "print(f\"\\nDimensions des tenseurs:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  X_val: {X_val.shape}\")\n",
    "print(f\"  y_val: {y_val.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "print(f\"  y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cr√©ation et Entra√Ænement du Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du mod√®le LSTM\n",
    "print(\"Cr√©ation du mod√®le LSTM...\")\n",
    "\n",
    "# Configuration du mod√®le\n",
    "model_config = {\n",
    "    'input_size': X_train.shape[2],\n",
    "    'hidden_size': params.get('hidden_size', 128),\n",
    "    'num_layers': params.get('num_layers', 2),\n",
    "    'output_size': 1,  # Pr√©diction du prix\n",
    "    'dropout': params.get('dropout', 0.2),\n",
    "    'bidirectional': params.get('bidirectional', False),\n",
    "    'batch_first': params.get('batch_first', True),\n",
    "    'bias': params.get('bias', True),\n",
    "    'batch_normalization': params.get('batch_normalization', False),\n",
    "    'layer_normalization': params.get('layer_normalization', False),\n",
    "    'recurrent_dropout': params.get('recurrent_dropout', 0.0),\n",
    "    'activation_function': params.get('activation_function', 'tanh'),\n",
    "    'recurrent_activation': params.get('recurrent_activation', 'sigmoid'),\n",
    "    'use_attention': params.get('use_attention', False),\n",
    "    'attention_heads': params.get('attention_heads', 4),\n",
    "    'attention_dropout': params.get('attention_dropout', 0.1)\n",
    "}\n",
    "\n",
    "# Cr√©ation du mod√®le\n",
    "model = LSTMModel(**model_config)\n",
    "\n",
    "print(\"Architecture du mod√®le:\")\n",
    "print(model)\n",
    "\n",
    "# Nombre total de param√®tres\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nNombre total de param√®tres: {total_params:,}\")\n",
    "print(f\"Param√®tres entra√Ænables: {trainable_params:,}\")\n",
    "\n",
    "# Test du mod√®le avec un batch\n",
    "test_input = torch.randn(1, SEQUENCE_LENGTH, X_train.shape[2])\n",
    "with torch.no_grad():\n",
    "    test_output = model(test_input)\n",
    "    print(f\"\\nTest du mod√®le - Input: {test_input.shape}, Output: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'entra√Æneur\n",
    "trainer_config = {\n",
    "    'model': model,\n",
    "    'learning_rate': params.get('learning_rate', 0.001),\n",
    "    'optimizer': params.get('optimizer', 'adam'),\n",
    "    'weight_decay': params.get('weight_decay', 0.0),\n",
    "    'gradient_clipping': params.get('gradient_clipping', None),\n",
    "    'use_lr_scheduler': params.get('use_lr_scheduler', False),\n",
    "    'lr_scheduler_type': params.get('lr_scheduler_type', 'step'),\n",
    "    'lr_scheduler_patience': params.get('lr_scheduler_patience', 10),\n",
    "    'lr_scheduler_factor': params.get('lr_scheduler_factor', 0.1),\n",
    "    'use_early_stopping': params.get('use_early_stopping', False),\n",
    "    'early_stopping_patience': params.get('early_stopping_patience', 10),\n",
    "    'early_stopping_min_delta': params.get('early_stopping_min_delta', 0.0),\n",
    "    'mixed_precision': params.get('mixed_precision', False),\n",
    "    'log_training_metrics': params.get('log_training_metrics', True),\n",
    "    'verbose': params.get('verbose_training', True)\n",
    "}\n",
    "\n",
    "# Cr√©ation de l'entra√Æneur\n",
    "trainer = LSTMTrainer(**trainer_config)\n",
    "\n",
    "print(\"Configuration de l'entra√Æneur:\")\n",
    "for key, value in trainer_config.items():\n",
    "    if key != 'model':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nEntra√Æneur cr√©√© avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion des donn√©es en tenseurs PyTorch\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Conversion en tenseurs\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)  # Ajout dimension pour la sortie\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_tensor = torch.FloatTensor(y_val).unsqueeze(1)\n",
    "\n",
    "# Cr√©ation des datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Configuration des DataLoaders\n",
    "batch_size = params.get('batch_size', 32)\n",
    "shuffle_training = params.get('shuffle_training', True)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=shuffle_training,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"DataLoaders cr√©√©s:\")\n",
    "print(f\"  Train batches: {len(train_loader)} (batch size: {batch_size})\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  Shuffle training: {shuffle_training}\")\n",
    "\n",
    "# Test d'un batch\n",
    "for batch_X, batch_y in train_loader:\n",
    "    print(f\"\\nDimensions d'un batch:\")\n",
    "    print(f\"  X: {batch_X.shape}\")\n",
    "    print(f\"  y: {batch_y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entra√Ænement du Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement du mod√®le\n",
    "print(\"D√©but de l'entra√Ænement du mod√®le LSTM...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Param√®tres d'entra√Ænement\n",
    "epochs = params.get('epochs', 50)\n",
    "save_frequency = params.get('save_model_frequency', 10)\n",
    "\n",
    "# Historique de l'entra√Ænement\n",
    "training_history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_mae': [],\n",
    "    'val_mae': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "# Temps d'entra√Ænement\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Boucle d'entra√Ænement\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    train_loss, train_mae = trainer.train_epoch(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_mae = trainer.validate(val_loader)\n",
    "    \n",
    "    # Mise √† jour de l'historique\n",
    "    training_history['train_loss'].append(train_loss)\n",
    "    training_history['val_loss'].append(val_loss)\n",
    "    training_history['train_mae'].append(train_mae)\n",
    "    training_history['val_mae'].append(val_mae)\n",
    "    \n",
    "    # R√©cup√©ration du learning rate actuel\n",
    "    current_lr = trainer.get_current_lr()\n",
    "    training_history['learning_rates'].append(current_lr)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Affichage des m√©triques\n",
    "    if (epoch + 1) % 1 == 0:  # Afficher √† chaque epoch\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Temps: {epoch_time:.1f}s\")\n",
    "        print(f\"  Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "        print(f\"  Train MAE: {train_mae:.6f} | Val MAE: {val_mae:.6f}\")\n",
    "        print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # V√©rification de l'early stopping\n",
    "        if trainer.early_stopping_triggered():\n",
    "            print(f\"  ‚Üí Early stopping d√©clench√© apr√®s {epoch+1} epochs!\")\n",
    "            break\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Sauvegarde p√©riodique\n",
    "    if (epoch + 1) % save_frequency == 0:\n",
    "        checkpoint_path = f'lstm_model_epoch_{epoch+1}.pth'\n",
    "        trainer.save_checkpoint(checkpoint_path, epoch+1, training_history)\n",
    "        print(f\"  ‚Üí Mod√®le sauvegard√©: {checkpoint_path}\")\n",
    "\n",
    "# Temps total d'entra√Ænement\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nEntra√Ænement termin√© en {total_time:.1f} secondes\")\n",
    "print(f\"Meilleure validation loss: {min(training_history['val_loss']):.6f}\")\n",
    "print(f\"Epoch avec la meilleure validation: {np.argmin(training_history['val_loss'])+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisation des R√©sultats d'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'historique d'entra√Ænement\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Loss\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(training_history['train_loss'], label='Train Loss', alpha=0.8)\n",
    "plt.plot(training_history['val_loss'], label='Validation Loss', alpha=0.8)\n",
    "plt.title('√âvolution de la Loss pendant l\\'entra√Ænement')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: MAE\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(training_history['train_mae'], label='Train MAE', alpha=0.8)\n",
    "plt.plot(training_history['val_mae'], label='Validation MAE', alpha=0.8)\n",
    "plt.title('√âvolution du MAE pendant l\\'entra√Ænement')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Learning Rate\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(training_history['learning_rates'], label='Learning Rate', alpha=0.8, color='green')\n",
    "plt.title('√âvolution du Learning Rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# R√©sum√© des performances\n",
    "print(\"R√âSUM√â DES PERFORMANCES:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Meilleure Train Loss: {min(training_history['train_loss']):.6f}\")\n",
    "print(f\"Meilleure Validation Loss: {min(training_history['val_loss']):.6f}\")\n",
    "print(f\"Meilleur Train MAE: {min(training_history['train_mae']):.6f}\")\n",
    "print(f\"Meilleur Validation MAE: {min(training_history['val_mae']):.6f}\")\n",
    "print(f\"Overfitting (derni√®re epoch): {training_history['val_loss'][-1] - training_history['train_loss'][-1]:.6f}\")\n",
    "print(f\"Learning Rate final: {training_history['learning_rates'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test du Mod√®le et √âvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©paration des donn√©es de test\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "\n",
    "# Pr√©dictions sur le set de test\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "\n",
    "# Conversion en numpy\n",
    "y_test_pred = test_predictions.numpy().squeeze()\n",
    "y_test_true = y_test_tensor.numpy().squeeze()\n",
    "\n",
    "# Calcul des m√©triques de test\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "test_mse = mean_squared_error(y_test_true, y_test_pred)\n",
    "test_mae = mean_absolute_error(y_test_true, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test_true, y_test_pred)\n",
    "test_mape = np.mean(np.abs((y_test_true - y_test_pred) / y_test_true)) * 100\n",
    "\n",
    "print(\"M√âTRIQUES DE TEST:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"MSE: {test_mse:.6f}\")\n",
    "print(f\"RMSE: {test_rmse:.6f}\")\n",
    "print(f\"MAE: {test_mae:.6f}\")\n",
    "print(f\"R¬≤: {test_r2:.6f}\")\n",
    "print(f\"MAPE: {test_mape:.2f}%\")\n",
    "\n",
    "# Comparaison avec une baseline (pr√©dire la derni√®re valeur connue)\n",
    "baseline_pred = X_test[:, -1, 0]  # Derni√®re valeur de close pour chaque s√©quence\n",
    "baseline_mae = mean_absolute_error(y_test_true, baseline_pred)\n",
    "baseline_r2 = r2_score(y_test_true, baseline_pred)\n",
    "\n",
    "print(f\"\\nComparaison avec baseline (derni√®re valeur):\")\n",
    "print(f\"  Baseline MAE: {baseline_mae:.6f}\")\n",
    "print(f\"  Am√©lioration MAE: {((baseline_mae - test_mae) / baseline_mae * 100):.2f}%\")\n",
    "print(f\"  Baseline R¬≤: {baseline_r2:.6f}\")\n",
    "print(f\"  Am√©lioration R¬≤: {test_r2 - baseline_r2:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des pr√©dictions vs valeurs r√©elles\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Pr√©dictions vs R√©el\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(y_test_true, y_test_pred, alpha=0.6)\n",
    "plt.plot([y_test_true.min(), y_test_true.max()], [y_test_true.min(), y_test_true.max()], 'r--', lw=2)\n",
    "plt.xlabel('Valeurs R√©elles')\n",
    "plt.ylabel('Pr√©dictions')\n",
    "plt.title(f'Pr√©dictions vs R√©elles (R¬≤ = {test_r2:.4f})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: S√©ries temporelles\n",
    "plt.subplot(2, 2, 2)\n",
    "n_points = min(200, len(y_test_true))  # Limiter pour la visibilit√©\n",
    "plt.plot(y_test_true[:n_points], label='Valeurs R√©elles', alpha=0.8)\n",
    "plt.plot(y_test_pred[:n_points], label='Pr√©dictions', alpha=0.8)\n",
    "plt.xlabel('Temps')\n",
    "plt.ylabel('Prix')\n",
    "plt.title(f'Pr√©dictions vs R√©elles (n={n_points})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: R√©sidus\n",
    "plt.subplot(2, 2, 3)\n",
    "residuals = y_test_true - y_test_pred\n",
    "plt.scatter(y_test_pred, residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Pr√©dictions')\n",
    "plt.ylabel('R√©sidus')\n",
    "plt.title('Analyse des R√©sidus')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Distribution des r√©sidus\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.hist(residuals, bins=50, alpha=0.7, density=True)\n",
    "plt.xlabel('R√©sidus')\n",
    "plt.ylabel('Densit√©')\n",
    "plt.title('Distribution des R√©sidus')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques des r√©sidus\n",
    "print(\"\\nSTATISTIQUES DES R√âSIDUS:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Moyenne: {residuals.mean():.6f}\")\n",
    "print(f\"√âcart-type: {residuals.std():.6f}\")\n",
    "print(f\"Min: {residuals.min():.6f}\")\n",
    "print(f\"Max: {residuals.max():.6f}\")\n",
    "print(f\"Skewness: {pd.Series(residuals).skew():.4f}\")\n",
    "print(f\"Kurtosis: {pd.Series(residuals).kurtosis():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sauvegarde du Mod√®le et Pr√©paration PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du mod√®le final\n",
    "final_model_path = 'lstm_model_final.pth'\n",
    "scaler_path = 'lstm_scaler.pkl'\n",
    "config_path = 'lstm_config_final.json'\n",
    "\n",
    "# Sauvegarde du mod√®le\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': model_config,\n",
    "    'training_history': training_history,\n",
    "    'test_metrics': {\n",
    "        'mse': test_mse,\n",
    "        'mae': test_mae,\n",
    "        'rmse': test_rmse,\n",
    "        'r2': test_r2,\n",
    "        'mape': test_mape\n",
    "    },\n",
    "    'scaler_info': {\n",
    "        'type': type(scaler).__name__,\n",
    "        'feature_names': ['close', 'open', 'high', 'low', 'volume', 'log_returns', 'sma_10', 'sma_20', 'rsi', 'volatility']\n",
    "    },\n",
    "    'sequence_length': SEQUENCE_LENGTH,\n",
    "    'prediction_horizon': PREDICTION_HORIZON,\n",
    "    'ticker': TICKER,\n",
    "    'training_period': {\n",
    "        'start': str(dates_train[0]),\n",
    "        'end': str(dates_train[-1])\n",
    "    }\n",
    "}, final_model_path)\n",
    "\n",
    "# Sauvegarde du scaler\n",
    "import pickle\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Sauvegarde de la configuration\n",
    "import json\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'model_config': model_config,\n",
    "        'training_config': params,\n",
    "        'data_config': {\n",
    "            'ticker': TICKER,\n",
    "            'sequence_length': SEQUENCE_LENGTH,\n",
    "            'prediction_horizon': PREDICTION_HORIZON,\n",
    "            'features': ['close', 'open', 'high', 'low', 'volume', 'log_returns', 'sma_10', 'sma_20', 'rsi', 'volatility']\n",
    "        },\n",
    "        'performance_metrics': {\n",
    "            'test_mse': test_mse,\n",
    "            'test_mae': test_mae,\n",
    "            'test_rmse': test_rmse,\n",
    "            'test_r2': test_r2,\n",
    "            'test_mape': test_mape\n",
    "        }\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"SAUVEGARDE TERMIN√âE:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Mod√®le: {final_model_path}\")\n",
    "print(f\"Scaler: {scaler_path}\")\n",
    "print(f\"Configuration: {config_path}\")\n",
    "print(f\"\\nLe mod√®le est pr√™t pour l'int√©gration PPO!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation d'une fonction de pr√©diction pour PPO\n",
    "def predict_with_lstm(model, scaler, data_sequence, device='cpu'):\n",
    "    \"\"\"\n",
    "    Fonction de pr√©diction optimis√©e pour l'utilisation avec PPO\n",
    "    \n",
    "    Args:\n",
    "        model: Le mod√®le LSTM entra√Æn√©\n",
    "        scaler: Le scaler utilis√© pour la normalisation\n",
    "        data_sequence: S√©quence de donn√©es (numpy array)\n",
    "        device: Dispositif ('cpu' ou 'cuda')\n",
    "    \n",
    "    Returns:\n",
    "        prediction: La pr√©diction du prix futur\n",
    "        confidence: Score de confiance (optionnel)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Pr√©paration des donn√©es\n",
    "        if len(data_sequence.shape) == 2:\n",
    "            data_sequence = data_sequence.reshape(1, *data_sequence.shape)\n",
    "        \n",
    "        # Normalisation si n√©cessaire\n",
    "        if data_sequence.max() > 10:  # Donn√©es non normalis√©es\n",
    "            # Appliquer le scaler (n√©cessite reshape)\n",
    "            original_shape = data_sequence.shape\n",
    "            flattened = data_sequence.reshape(-1, data_sequence.shape[-1])\n",
    "            normalized = scaler.transform(flattened)\n",
    "            data_sequence = normalized.reshape(original_shape)\n",
    "        \n",
    "        # Conversion en tenseur\n",
    "        data_tensor = torch.FloatTensor(data_sequence).to(device)\n",
    "        \n",
    "        # Pr√©diction\n",
    "        prediction = model(data_tensor)\n",
    "        \n",
    "        # Conversion numpy\n",
    "        prediction = prediction.cpu().numpy().squeeze()\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Test de la fonction de pr√©diction\n",
    "print(\"Test de la fonction de pr√©diction PPO:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test avec une s√©quence\n",
    "test_sequence = X_test[0:1]  # Premier √©chantillon\n",
    "prediction = predict_with_lstm(model, scaler, test_sequence)\n",
    "\n",
    "print(f\"S√©quence d'entr√©e shape: {test_sequence.shape}\")\n",
    "print(f\"Pr√©diction: {prediction:.4f}\")\n",
    "print(f\"Valeur r√©elle: {y_test[0]:.4f}\")\n",
    "print(f\"Erreur: {abs(prediction - y_test[0]):.4f}\")\n",
    "\n",
    "# Test avec donn√©es non normalis√©es (simulation PPO)\n",
    "raw_sequence = np.random.rand(SEQUENCE_LENGTH, X_test.shape[2]) * 100  # Donn√©es brutes\n",
    "prediction_raw = predict_with_lstm(model, scaler, raw_sequence)\n",
    "\n",
    "print(f\"\\nTest avec donn√©es brutes:\")\n",
    "print(f\"  S√©quence brute shape: {raw_sequence.shape}\")\n",
    "print(f\"  Pr√©diction: {prediction_raw:.4f}\")\n",
    "print(f\"  ‚Üí La fonction g√®re correctement la normalisation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. R√©sum√© et Prochaines √âtapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© final\n",
    "print(\"R√âSUM√â DE L'ENTRA√éNEMENT LSTM\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úì Mod√®le LSTM entra√Æn√© avec succ√®s\")\n",
    "print(f\"‚úì Architecture: {params.get('num_layers', 2)} couches, {params.get('hidden_size', 128)} hidden size\")\n",
    "print(f\"‚úì Bidirectionnel: {params.get('bidirectional', False)}\")\n",
    "print(f\"‚úì Batch Normalization: {params.get('batch_normalization', False)}\")\n",
    "print(f\"‚úì Dropout: {params.get('dropout', 0.2)}\")\n",
    "print(f\"‚úì Performance Test R¬≤: {test_r2:.4f}\")\n",
    "print(f\"‚úì Performance Test MAE: {test_mae:.6f}\")\n",
    "print(f\"‚úì Am√©lioration vs baseline: {((baseline_mae - test_mae) / baseline_mae * 100):.2f}%\")\n",
    "print(f\"‚úì Fonction de pr√©diction PPO cr√©√©e\")\n",
    "print(f\"‚úì Mod√®les sauvegard√©s et pr√™ts √† l'emploi\")\n",
    "\n",
    "print(\"\\nFICHIERS CR√â√âS:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"1. {final_model_path} - Mod√®le LSTM complet\")\n",
    "print(f\"2. {scaler_path} - Scaler pour normalisation\")\n",
    "print(f\"3. {config_path} - Configuration compl√®te\")\n",
    "\n",
    "print(\"\\nPROCHAINES √âTAPES POUR PPO:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"1. Charger le mod√®le LSTM dans train_minute_model_lstm.py\")\n",
    "print(\"2. Utiliser la fonction predict_with_lstm() pour obtenir des pr√©dictions\")\n",
    "print(\"3. Int√©grer les pr√©dictions dans l'environnement PPO\")\n",
    "print(\"4. Entra√Æner l'agent PPO avec les signaux LSTM\")\n",
    "print(\"5. √âvaluer les performances combin√©es LSTM+PPO\")\n",
    "\n",
    "print(\"\\nCODE D'INT√âGRATION PPO:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"# Dans train_minute_model_lstm.py, ajouter:\")\n",
    "print(\"# from lstm_model import LSTMModel\")\n",
    "print(\"# from lstm_config import LSTMConfig\")\n",
    "print(\"# import pickle\")\n",
    "print(\"# \")\n",
    "print(\"# # Charger le mod√®le\")\n",
    "print(\"# checkpoint = torch.load('lstm_model_final.pth')\")\n",
    "print(\"# model = LSTMModel(**checkpoint['model_config'])\")\n",
    "print(\"# model.load_state_dict(checkpoint['model_state_dict'])\")\n",
    "print(\"# \")\n",
    "print(\"# # Charger le scaler\")\n",
    "print(\"# with open('lstm_scaler.pkl', 'rb') as f:\")\n",
    "print(\"#     scaler = pickle.load(f)\")\n",
    "print(\"# \")\n",
    "print(\"# # Utiliser dans l'environnement PPO\")\n",
    "print(\"# lstm_prediction = predict_with_lstm(model, scaler, market_data_sequence)\")\n",
    "print(\"\\nLe mod√®le LSTM est maintenant pr√™t pour l'int√©gration PPO! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cr√©ation d'un Rapport de Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation d'un rapport complet\n",
    "report = f\"\"\"\n",
    "RAPPORT D'ENTRA√éNEMENT LSTM - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "================================================================\n",
    "\n",
    "CONFIGURATION:\n",
    "- Ticker: {TICKER}\n",
    "- P√©riode: {START_DATE} √† {END_DATE}\n",
    "- S√©quence: {SEQUENCE_LENGTH} jours\n",
    "- Horizon: {PREDICTION_HORIZON} jours\n",
    "- Mode: {CONFIG_MODE.upper()}\n",
    "\n",
    "ARCHITECTURE:\n",
    "- Hidden size: {params.get('hidden_size', 128)}\n",
    "- Nombre de couches: {params.get('num_layers', 2)}\n",
    "- Bidirectionnel: {params.get('bidirectional', False)}\n",
    "- Dropout: {params.get('dropout', 0.2)}\n",
    "- Batch Normalization: {params.get('batch_normalization', False)}\n",
    "\n",
    "PERFORMANCES:\n",
    "- Test R¬≤: {test_r2:.4f}\n",
    "- Test MAE: {test_mae:.6f}\n",
    "- Test RMSE: {test_rmse:.6f}\n",
    "- Test MAPE: {test_mape:.2f}%\n",
    "- Am√©lioration vs baseline: {((baseline_mae - test_mae) / baseline_mae * 100):.2f}%\n",
    "\n",
    "FICHIERS CR√â√âS:\n",
    "- Mod√®le: {final_model_path}\n",
    "- Scaler: {scaler_path}\n",
    "- Config: {config_path}\n",
    "\"\"\"\n",
    "\n",
    "# Sauvegarde du rapport\n",
    "with open('lstm_training_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"RAPPORT SAUVEGARD√â:\")\n",
    "print(\"=\"*30)\n",
    "print(\"Le rapport complet a √©t√© sauvegard√© dans 'lstm_training_report.txt'\")\n",
    "print(\"\\nLe mod√®le LSTM est maintenant pr√™t pour l'int√©gration avec PPO!\")\n",
    "print(\"Vous pouvez utiliser les fichiers cr√©√©s pour charger le mod√®le dans\")\n",
    "print(\"votre environnement PPO et commencer l'entra√Ænement de l'agent.\")\n",
    "print(\"\\nBonne chance avec votre trading automatique LSTM+PPO! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
